<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>w271: Categorical, Time Series, and Panel Regression</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jeffrey Yau" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# w271: Categorical, Time Series, and Panel Regression
]
.subtitle[
## Autoregressive and Moving Average Models
]
.author[
### Jeffrey Yau
]
.institute[
### UC Berkeley, School of Information
]
.date[
### Updated: 2022-06-21
]

---




class: inverse, center, middle 

# Autoregressive Models, Part III: Expression in Lag Operators 

---
# Backshift Operators: An Introduction
## Backshift Operator, Part I 

A very useful concept is the backshift operator because it (and its associated characteristic polynomials) can be used to study the properties of `\(AR(p)\)` models, and `\(ARIMA(p)\)` models in general. 

`$$\phi(B) = 1 - \phi_{1}B - \phi_{2}B^{2} - \cdots - \phi_{p-1}B^{p-1} - \phi_{p}B^{p}$$`

Using the backshift operator, the AR model can be re-written as: 

`$$\phi(B)x_{t} = \omega_{t}$$` 

The equation `\(\phi(B) = 0\)` is called a *characteristic equation*, and it provides a tool to check if a process is stationary. 

---
# Backshift Operators: An Introduction
## Is a process stationary? 

If the absolute value of roots of the characteristic equation *all* exceed `\(1\)`, then the process `\(x_{t}\)` is *stationary*. 

---
# Examples of Using the Backshift Operator
## Example 1

- Suppose that there is a random walk model `\(x_{t} = x_{t_1} + \omega_{t}\)`. 
- This has the characteristic equation `\(\phi = 1\)` and `\(\theta = 1 - B\)` with root `\(B=1\)`. 
- **Therefore**, this is a non-stationary process because the root does not exceed `\(1\)`.

---
# Examples of Using the Backshift Operator 
## Example 2 

- Suppose that there is an `\(AR(1)\)` model `\(x_{t} = \frac{1}{2}x_{t-1} + \omega_{t}\)`. 
- This has the characteristic equation `\(1 - \frac{1}{2}B = 0\)`, and the root of `\(B=2\)` which is greater than `\(1\)`. 
- **Therefore**, this `\(AR(1)\)` model is stationary. 

---
# Examples of Using the Backshift Operator 
## Example 3 

- Suppose that there is an `\(AR(2)\)` model `\(x_{t} = x_{t-1} - \frac{1}{4}x_{t-2} + \omega_{t}.\)` 
- Expressed using the backshift operator, 

`$$\begin{aligned} 
\omega_{t} &amp;= \frac{1}{4}(B^{2} - 4^B + 4)x_{t} \\ 
  &amp;= \frac{1}{4}(B-2)^{2}x_{t}
\end{aligned}$$`

- The corresponding characteristic equation is `\(\phi(B) = \frac{1}{4}(B-2)^{2} = 0\)`, so the root is `\(B = 2\)` which is greater than `\(1\)`. 
- **Therefore**, this `\(AR(2)\)` model is stationary. 

---
# Examples of Using the Backshift Operator 
## Example 4

- Suppose that there is an `\(AR(2)\)` model `\(x_{t} = \frac{1}{2}x_{t-1} + \frac{1}{2}x_{t-2} + \omega_{t}\)`. 
- Expressed using the backshift operator, `\(-\frac{1}{2}(B^{2} + B - 2)x_{t} = \omega_{t}\)`. 
- The corresponding polynomial `\(\phi(B) = -\frac{1}{2}(B-1)(B+2)\)` has roots `\(B = \{1, -2\}\)`. 
- **Therefore**, with the unit root `\(B=1\)` this `\(AR(2)\)` model is *non-stationary*.

---
class: inverse, center, middle 
# Intuition of the Properties of the General AR(p) Models

---
# Autoregressive Models in Lag Operators 

For a (weakly) stationary `\(AR(p)\)` model, its lag operator presentation is: 

`$$\Phi(L)x_{t} = \alpha + \epsilon_{t},$$`

where: 

`$$\Phi(L) = 1 - \phi L - \dots - \phi_{p}L^{p}$$`

- Recall that the stationarity condition for the `\(AR(p)\)` model is that the roots of the characteristic polynomial `\(Phi(z)\)` all lie **outside** the unit circle. 
- Equivalently, the inverse of the roots of `\(z^{p} - \phi_{1}z^{p-1} - \dots - \phi_{p}\)` all lie **inside** the unit circle. 

The expectation of this model is 

`$$E(x_{t}) = \frac{\alpha}{1 - \phi_{1} - \dots - \phi_{p}}.$$` 

The autocovariance function of the general `\(AR(p)\)` model that be derived using the *Yule-Walker* equations. 

---
# Key Properties of the General AR(p) Model, Part I 
The general properties of an `\(AR(1)\)` model carry through to an `\(AR(p)\)` model. 

1. **Stationarity Condition**: An `\(AR(p)\)` process is covariance stationary if and only if the **inverse** of all roots of the autoregressive lag operator polynomial `\(\Phi(B)\)` are *inside* the unit circle. 
2. **ACF**: The autocorrelation function for the `\(AR(p)\)` process decays gradually with displacement. 
3. **PACF**: The partial autocorrelation function has a sharp cut-off at **displacement p**. 

---
# Key Properties of the General AR(p) Model, Part II
**However** there are differences between the genearal `\(AR(p)\)` models and the `\(AR(1)\)` models: 

1. Models with higher autoregressive order allow for richer dynamics. As a result, the autocorrelation function displays a wider "variety" of patterns. 
  - It might display damped, monotonic decays as in the `\(AR(1)\)` case with a positive coefficient; or, 
  - It might have a damped oscillation that `\(AR(1)\)` processes cannot have unless their coefficient are negative. 
2. The richer patterns of the *ACF* from the higher-order autoregressive models can mimic a wider range of cyclical patterns. 

---
class: inverse, center, middle 
# Simulation of AR(2) Models 

---
# Use An AR(2) Model to Illustrate Properties, Part I 

Consider an `\(AR(2)\)` model that has the following specification: 

`$$y_{t} = 1.5 y_{t-1} - 0.9 y_{t-2} + \epsilon_{t}$$` 

The corresponding lag operator polynomial is: 

`$$1 - 1.5B - 0.9 B^{2}$$`

The roots of this polynomial can found in R using the `polyroot` function, and are the following two complex conjugate roots: 


```r
lag_operator &lt;- c(1, -1.5, 0.9)
polyroot(lag_operator)
```

```
## [1] 0.8333+0.6455i 0.8333-0.6455i
```

---
# Use An AR(2) Model to Illustrate Properties, Part I 

The absolute values of these roots are: 


```r
abs(polyroot(lag_operator))
```

```
## [1] 1.054 1.054
```

And, the inverse of these absolute roots are: 


```r
1 / abs(polyroot(lag_operator))
```

```
## [1] 0.9487 0.9487
```

Both of these roots are *close* to `\(1\)`, but they are inside the unit circle, therefor the process is **covariance stationary**. 

---
# Autocorrelation of an AR(2) process
The autocorrelation function of an `\(AR(2)\)` process is: 

`$$\begin{aligned}
  \rho(0) &amp;= 1 \\ 
  \rho(1) &amp;= \frac{\phi_{1}}{1-\phi_{2}} \\ 
  \rho(2) &amp;= \phi_{1}\rho(\tau - 1) + \phi_{2}\rho(\tau - 2) &amp; \tau = 2, 3, \dots
\end{aligned}$$`

---
# Use An AR(2) Model to Illustrate the Properties, Part II

- Because the roots are complex, the autocorrelation function oscillates 
- Because the root are close to one, the autocorrelation function oscillates slowly. 




![](unit_08_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;

---
# # Use An AR(2) Model to Illustrate the Properties, Part III

This histogram looks reasonably symmetric: 

![](unit_08_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;

---
# Use An AR(2) Model to Illustrate the Properties, Part IV

- The time series plot shows that the series displays fluctuations 
- The magnitude of these fluctuations change over time. 

![](unit_08_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;

---
# Use An AR(2) Model to Illustrate the Properties, Part V 

- As in the `\(AR(1)\)` model where the PACF has a sharp cut-off at **displacement 1** , the PACF of the `\(AR(2)\)` process has a strong cut-off at **displacement 2**. 

![](unit_08_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;

---
class: inverse, middle, center
# Model Estimation and Model Selection 

---
# Estimation, Example 1: AR(1), Part I

- Let's apply an `\(AR\)` model to the series we simulated using an `\(AR(1)\)` process of the following specification: 

`$$x_{t} - \mu = 0.7(x_{t-1} - \mu) + \omega_{t},$$`

where `\(\mu\)` is the mean of the series. 


```r
x &lt;- arima.sim(
  n=1000, 
  model = list(ar=0.7, ma=0)) %&gt;% 
  as_tsibble(index = index)
```


```
## Rows: 1,000
## Columns: 2
## $ index &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1…
## $ value &lt;dbl&gt; -1.2799, -1.9968, -1.0408, -0.9657, -0.4421, 0.8107, 0.4116, 0.4…
```

```
##      index          value       
##  Min.   :   1   Min.   :-4.484  
##  1st Qu.: 251   1st Qu.:-0.815  
##  Median : 500   Median : 0.059  
##  Mean   : 500   Mean   : 0.048  
##  3rd Qu.: 750   3rd Qu.: 0.935  
##  Max.   :1000   Max.   : 4.292
```

---
# Estimation, Example 1: AR(1), Part II

- Examine the simulated series by visualizing its distribution (using a histogram), its dynamics (using the time series plot), and its dependence structure using ACF and PACF graphs. 

![](unit_08_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;

---
# Estimation, Example 1: AR(1), Part III

- This estimation uses the `ar()` function which estimates a `\(AR\)` model of the following form: 

`$$x_{t}-\mu = \alpha_{1}(x_{t-1} - \mu) + \dots + \alpha_{p}(x_{t-p} - \mu) + \omega_{t}$$`. 

    ar(x, aic = TRUE, order.max = NULL,
       method = c("yule-walker", "burg", "ols", "mle", "yw"),
       na.action, series, ...)
       
- By default, `ar()` uses `\(AIC\)` as the selection criteria 
- Recall that `\(AIC\)` is a goodness of fit measure that penalizes the number of parameters. (We will discuss this on the next slide). 

---
# Estimation, Example 1: AR(1), Part IV


```r
ar_model &lt;- x %$%
  ar(value, method = 'mle')
```

From the model that is fitted, there are several objects of interest that can be accessed. We will not print all of them here, but the *structure* can be viewed using the `str` call, which will provide the names of the list objects that might be of interest. 


```r
# str(ar_model)
ar_model$order
```

```
## [1] 1
```

```r
ar_model$ar
```

```
## [1] 0.6696
```

---
# Estimation, Example 1: AR(1), Part V 


```r
sqrt(ar_model$asy.var.coef)
```

```
##         [,1]
## [1,] 0.02351
```

```r
ar_model$aic
```

```
##        0        1        2        3        4        5        6        7 
## 590.5591   0.0000   0.7343   2.4281   3.1921   5.1462   7.0451   8.9261 
##        8        9       10       11       12 
##   7.8241   9.3768   9.8720  11.3174  12.8061
```

- Note that the AIC that are shown in the `ar_model$aic` are all displayed, but they are shown as a **difference** between the model with the *lowest* `\(AIC\)` and the `\(AIC\)` of the numbered model. 

---
# Estimation, Example 1: AR(1), Part VI

Notice that because we have produced an estimate of the `order` of the AR model, as well as the parameter estimate, **and** an estimate of the standard error of the model, is is possible to produce a confidence interval for this parameter estimate: 


```r
critical_values &lt;- qt(c(0.025, 0.975), df = nrow(x))
se &lt;- c(sqrt(ar_model$asy.var.coef)) # this c() is to convert the 
                                     # type to avoid a warning

ar_model$ar + (critical_values * se)
```

```
## [1] 0.6234 0.7157
```

---
# Estimation, Example 1: AR(1), Part VII

Based on this *particular* sample path: 

- The estimated parameter estimate is 0.6696. 
- The `\(95\)` **% Confidence Interval** is (0.623, 0.716) 
- Taking care to interpret this frequentist confidence interval, we know that this estimation process would produce a CI that **actually does** contain the *unknown, true* order parameter `\(95\)` out of `\(100\)` times. 

---
class: inverse, center, middle 
# Model Diagnostics and Assumption Testing 

---
# Estimation 

Model assumption and diagnostic testing; 

1. `\(AR\)` models that random components resembling that of white noise. 

&gt; **Question to evaluate**: Do the estimated residuals look like the realizations generated by a white noice process? 

2. We are interested in `\(AR\)` models. 

&gt; **Question to evaluate**: Is our estimated `\(AR\)` model stationary (at least statistically)? 

---
# Model Assumption Diagnosis and Testing 

Do the estimated residuals look like the realizations generated by a white noice process? 



![](unit_08_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;

---
class: inverse, center, middle
# Example 2: Estimation, Model Selection, Model Diagnostics, and Assumption Testing 

---
# Example 2: The True Data Generating Process 

- Let's apply an `\(AR\)` model to the more complicated series of data that we simulated in `sim_data`
- The *true* (the population model that our simulation is sampling from) modle that generated this data has the following *functional form*: 

`$$y_{t} = 1.5 y_{t-1} - 0.9y_{t-2} + \epsilon_{t}$$` 

- Because we have already examined the series, proceed direction to estimation (without EDA this time)
- For this estimation, suspend reality and suppose that we do not have knowledge of the underlying **data generaeting process (DGP)** but based on the EDA that we've done, we decide to try using an `\(AR\)` model. 

---
# Example 2: R commands 

- We will us the `ar` function to select the model with the lowest `\(AIC\)`. 
- Then, we will examine the `order`, `ar`, `aic`, and `asy.var.coef`.
- *NB: Because of stochastic errors, the run in the slides might be different when you run it than were shown in the lecture*.


```r
ar_sim &lt;- ar(sim_data)
ar_sim$order
```

```
## [1] 3
```

```r
ar_sim$ar
```

```
## [1]  1.4212 -0.7360 -0.1009
```

```r
ar_sim$aic[1:12]
```

```
##        0        1        2        3        4        5        6        7 
## 2584.174 1564.484    8.240    0.000    1.880    3.875    5.231    7.189 
##        8        9       10       11 
##    9.055   11.055   12.700   14.315
```

---
# Example 2: Points to Note 

1. This `ar` call estiamted many different AR models, with orders from `\(1 - p\)`. It is possible to change the maximum order using the argument `order.max=`. 
2. The *best* model (in terms of smallest AIC) depends in some part on the sample. In the async, when Jeffrey runs this code, the `\(AR(5)\)` is the best, even though the **true** DGP is an `\(AR(2)\)`. Is this an *error*? No, it is a the result of a sample.
3. The `\(AIC\)` of some of these models is *huge* but several are within the "same ballpark" as the true DGP. 

---
# Example 2: Model Diagnostics 

Examine the reisduals: 

- Notice that the first several observations in the estimated residual serires are missing. This is because `\(p\)` residuals are excluded from the best fitting, order `\(p\)` model. 
- For this reason, the `\(ACF\)` and `\(PACF\)` are excluded from plots. 

---
# Example 2: Model Diagnostics 

Good news! The residuals look like white noise series. 



![](unit_08_files/figure-html/unnamed-chunk-16-1.png)&lt;!-- --&gt;

---
# Making a Final Determination, Part I 

- In the run in the async, the `\(AR(5)\)` model produces the lowest `\(AIC\)`. But this is a single model summary. 

## Considerations 

1. Are the the differences between the `\(AIC\)` from different models "really that big"? 
2. If the focus is "in-sample fit", we might consider more than `\(AIC\)`, for example `\(BIC\)`. 
3. If we were to focus on "on-of-sample fit" we might learn that a different model is performing better. Or, that in- vs. out-of sample fit might prefer different models. 
4. **Most importantly**: We need to consider, "what is the goal of this model?" and, "What question are we trying to answer?" when we are making a determination about the "best" model. 

---
# Making a Final Determination, Part II 

## Objective: Forecasting 
If the goal is forecasting, then model performance should evaluate forecast errors to compare among models. 

- How long is the time horizon? 
- Is the forecast short-term or long-term? 
- How long is the sample that we want to use to estimate the model? Do we *always* want to use the entire sample? Or, do we want to privilege newer (or older) behavior? 
- Should we leave a subset of the *already observed* samples for out-of-sample tests? 
  - Creating train/test partitions is *very* common in ML applications.
  - In time series analysis, data is often limited (i.e. there may be 2,000 observations, not 2,000,000), and how much sample to leave out for testing requires a discussion between the data scientists/modelerse and the business stakeholders.
  
---
class: inverse, center, middle
# Example 3: Global Surface Temperature 

---
# Example 3: Global Surface Temperature 

- Global surface temperature *change* measured as an annual average. 
- Annual series measured from 1880 - 2009. 
- Let's fit an `\(AR\)` model to this series. 


```
## Rows: 130
## Columns: 2
## $ index &lt;dbl&gt; 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890…
## $ value &lt;dbl&gt; -0.28, -0.21, -0.26, -0.27, -0.32, -0.32, -0.29, -0.36, -0.27, -…
```

---
# Example 3: Global Surface Temperature 
## EDA: Histogram


```r
gtemp %&gt;% 
  ggplot(aes(x=value)) + 
  geom_histogram(bins = 10) + 
  labs(title = 'Average Annual Change in Global Surface Temperature')
```

![](unit_08_files/figure-html/unnamed-chunk-18-1.png)&lt;!-- --&gt;

---
# Example 3: Global Surface Temperature 
## EDA: Time Series Plot 


```
## Plot variable not specified, automatically selected `.vars = value`
```

![](unit_08_files/figure-html/unnamed-chunk-19-1.png)&lt;!-- --&gt;

---
# Example 3: Global Surface Temperature 
## EDA: ACF Plot 

![](unit_08_files/figure-html/unnamed-chunk-20-1.png)&lt;!-- --&gt;

---
# Example 3: Global Surface Temperature 
## EDA: PACF Plot 

![](unit_08_files/figure-html/unnamed-chunk-21-1.png)&lt;!-- --&gt;

---
# Example 3: Global Surface Temperature 
## Observing a Problem 

- There is a trend that needs to be accounted for before fitting an `\(AR\)` model. 
- We have not yet studied how to handle these trends. 
- What if we proceed without addressing this problem? 


```r
global_temperature_change_model &lt;- ar(gtemp$value, method = 'mle')
```

```
## Warning in arima0(x, order = c(i, 0L, 0L), include.mean = demean): possible
## convergence problem: optim gave code = 1

## Warning in arima0(x, order = c(i, 0L, 0L), include.mean = demean): possible
## convergence problem: optim gave code = 1

## Warning in arima0(x, order = c(i, 0L, 0L), include.mean = demean): possible
## convergence problem: optim gave code = 1
```

- This is "good" news. We failed to address the trend, and the model could not converge and fails. This is a desired failure. 

---
class: inverse, center, middle
# Moving Average Models 

---
# Mathmatical Formulation of a MA Process

- A **moving average** model of order `\(q\)` is al inear combination of the *current* and *past* `\(q\)` white noises: 

`$$x_{t} = \omega_{t} + \theta_{1}\omega_{t-1} + \dots + \theta_{t-q}\omega_{t-q}, \text{ where,}$$` 
- `\(\{\omega_{t}\}\)` are a white noise sequence where each `\(\omega\)` has **zero mean** and **common variance** `\(\sigma_{\omega}^{2}\)`. 
- `\(x_{t}\)` is a **demeaned series**, so that each `\(x_{t} = x_{t}^{*} - \mu\)`

Like `\(AR\)` models, these `\(MA\)` models can be experessed using the backshift operator, 

`$$\begin{aligned} 
\tilde{x}_{t} &amp;= (1 + \theta_{1}B^{1} + \theta_{2}B^{2} + \dots + \theta_{q}B^{q}) \\ 
  &amp;= \theta_{q}(B)\omega_{t}, \text{ where,}
\end{aligned}$$`
`\(\theta_{q}()\)` is a polynomial of the order `\(q\)`. 

- Since the `\(MA(q)\)` process is a **finite linear combination** of **white noises**, the movign average process is **stationary** with constant *mean*, *variance*, and *autocovariance*. 

## The stationary condition for a MA process is met regardless of the values of its parameters. 

--- 
# Properties of a MA Process 

- A `\(MA\)` process is a function of both *current* and *past* shocks. 
  - These shocks are *theoretical* 
  - These shocks are *unobservable*
  - Think of these in the same way that we think of a `\(pdf\)` in probability theory

## We cannot use MA models for forecasing: 
- Forecasting generally requires an established statistical relationship between current and past values. 
- If the relationship is only through the shocks, there's nothing that can be utilized! 

## Can we express a MA model as a function of current and past observable values? 
- This gives rise to the concept of **invertability**. 

---
# Invertability, Part I 

- If a `\(MA\)` process is **invertable** then it can be expressed as a function of current shock, and lagged values of the series. 
- We call this form of expression of `\(MA\)` models the **autoregressive representation.** 

## How do we know if a MA process is invertable? 
- A `\(MA(q)\)` process is *invertable* when the absolute value of the roots of `\(\theta_{q}(B)\)` **all** exceed `\(1\)`. 
- This invertability condition is necessary because of its *practical* importance. 
- If a `\(MA\)` model is invertable, then it can be expressed in its `\(AR\)` representation. 

---
# Invertabiltiy, Part II

- Forecasting requires an *estimable*, *statistical* linkage between past observations and present observations.
  - `\(MA\)` models do not have such a relationship -- values are linked only through random shocks. 
  - But, with the `\(AR\)` representation, a data scientist/modeler can produce a *real-world* forecast despite the `\(MA\)` limitations
- With this linkage (or a model), the data scientist can extrapolate to form a forecast based on prsent and past observations. 
- This leads us to focus on invertable processes. 

---
class: inverse, center, middle
# Invertability of MA Models 

---
# The Invertability Condition, Part I 

- Consider the following `\(MA(1)\)` model: 

`$$\begin{aligned} 
  y_{t}        &amp;= \epsilon_{t} + \theta\epsilon_{t-1} \\ 
  \epsilon_{t} &amp;\sim WN(0, \sigma_{\omega}^{2}).
\end{aligned}$$`

From here, we can solve for the shocks (or innovations): 

`$$\epsilon_{t} = y_{t} - \theta_{\epsilon_{t-1}}$$`

And then, using recursive substitution, 

`$$\begin{aligned} 
  \epsilon_{t-1} &amp;= y_{t-1} - \theta_{\epsilon_{t-2}} \\
  \epsilon_{t-2} &amp;= y_{t-2} - \theta_{\epsilon_{t-3}} \\ 
  \epsilon_{t-3} &amp;= y_{t-3} - \theta_{\epsilon_{t-4}} \\ 
    \vdots &amp;= \vdots
\end{aligned}$$`

---
# The Invertability Condition, Part II

Substituting, we obtain:  

`$$y_{t} = \epsilon_{t} + \theta y_{t-1} + \theta^{2} y_{t-2} + \theta^{3}y_{t-3} + \dots$$` 

Rearranging and then using the *backshift* operator: 

`$$\begin{aligned} 
  y_{t} - \theta y_{t-1} - \theta^{2} y_{t-2} - \theta^{3} y_{t-3} - \dots &amp;= \epsilon_{t} \\ 
  (1 - \theta B - \theta^{2} B^{2} - \theta^{3} B^{3})y_{t} &amp;= \epsilon_{t}
\end{aligned}$$`

The *infinite autoregressive representation* of a `\(MA(1)\)` model can be expressed as: 

`$$\frac{1}{1+\theta B} = \epsilon_{t}$$` 

- Expressed using the *backshift* operator, the infinite sequence of `\(\{\theta^{i}\}_{i=1}^{\infty}\)` converges *iff* `\(\left|\theta\right| &lt; 1\)`. 
- **This is the invertable condition for the `\(MA(1)\)` model.**

---
# Expectation and Variance of the MA Model 
## Expectation

- The `\(MA(q)\)` model has a constant expectation equal to: 

`$$E[x_{t}] = \sum_{j=0}^{q}\theta_{j} E[\omega_{t-j}] = 0$$` 

where `\(\theta_{0} = 1\)`. 

## Variance
- The `\(MA(q)\)` model has a constant variance equal to: 

`$$V[x_{t}] = \sum_{j=0}^{q} \theta_{j}V[\omega_{t-j}] = (1 + \beta_{1} + \beta_{2} + \dots + \beta_{q})\sigma_{\omega}^{2}$$`.

---
# Covariance of the MA Model 
## Covariance 

- Let `\(\gamma(h)\)` be the covariance between a point in time, `\(t\)`, and another point in time `\(h\)` periods in the future from `\(t\)` (i.e. `\(t+h\)`.) Then, 

`$$\gamma(h) \equiv Cov[x_{t}, x_{t+h}] = Cov\left[\sum_{j=0}^{q} \theta_{j}\omega_{t+h-j}, \sum_{k=0}^{q}\theta_{k}\omega_{t-k} \right].$$`

- With this variance and covariance, we can write down the autocorrelation function for any `\(k \geq 0:\)`

`$$\rho(k) = \begin{cases} 
  1, &amp; k = 0 \\ 
  \frac{\sum_{i=0}^{q-k}\beta_{i}\beta_{i+k}}{\sum_{i=0}^{q}\beta_{i}^{2}}, &amp; k \in \{1, 2, \dots, q\} \\ 
  0, &amp; k &gt; q,
  \end{cases}$$`

where we define `\(\beta_{0} = 1.\)`

---
# Working with a `\(MA(2)\)` Model 

- Let's take this formula "out for a drive" on a `\(MA(2)\)` model. 
- By assumption, this model has **mean** `\(\mu\)` and **variance** `\(\sigma_{\omega}^{2}(1 + \beta_{1}^{2} + \beta_{2}^{2})\)`, and an autocorrelation: 

`$$\rho(k) = \begin{cases} 
  1, &amp; k = 0 \\ 
  \frac{\beta_{1}(1 + \beta_{2})}{(1 + \beta_{1}^{2} + \beta_{2}^{2})}, &amp; k=1 \\ 
  \frac{\beta_{2}(1 + \beta_{2})}{(1 + \beta_{1}^{2} + \beta_{2}^{2})}, &amp; k=2 \\ 
  0, &amp; k &gt; 2
\end{cases}$$`

- There is perfect autocorrelation in the *same* period. 
- There is zero autocorrelation in periods *more distant* than the the process. 
- The autocorrelation scales with the strength of the relationship between all time periods that *could* affect outcomes (i.e. those that are less than the process order). 

---
class: inverse, center, middle
# Recap of MA Models' Key Properties

---
# The "Memory" of an MA Process, Part I 

## Autocovariance

`$$\begin{aligned}
  \gamma_{k} &amp;= E\left[(\omega_{t} + \theta_{1}\omega_{t-1} + \dots + \theta_{q}\omega_{t-q})(\omega_{t-k} + \theta_{1}\omega_{t-k-1} + \dots + \theta_{q}\omega_{t-k-q}) \right] \\ 
  &amp;= \theta_{k}E\left[\omega_{t-k}^{2} \right] + \theta_{1}\theta_{k+1}E\left[\omega_{t-k-1}^{2}\right] + \dots + \theta_{q-k}\theta_{q}E\left[\omega_{t-q}^{2} \right] 
\end{aligned}$$`

## Variance

`$$\gamma_{0} = \left(1 + \theta_{1}^{2} + \theta_{2}^{2} + \dots + \theta_{q}^{2} \right) \sigma_{\omega}^{2}$$`

## Autocovariance

`$$\gamma_{k} = \begin{cases} 
  (\theta_{k} + \theta_{1}\theta_{k+1} + \theta_{2}\theta_{k+2} + \dots + \theta_{q-k}\theta_{q})\sigma_{\omega}^{2}, &amp; k \in \{1, 2, \dots, q \} \\ 
  0, &amp; k &gt; q
\end{cases}$$`

## Autocorrelation

`$$\rho_{k} = \begin{cases} 
  \frac{\theta_{k} + \theta_{1}\theta_{k+1} + \theta_{2}\theta_{k+2} + \dots + \theta_{q-k}\theta_{q}}{1 + \theta_{1}^{2} + \theta_{2}^{2} + \dots + \theta_{q}^{2}}, &amp; k \in \{1, 2, \dots, q \} \\
  0, &amp; k &gt; q \\
\end{cases}$$`

---
# The "Memory" of a MA Process, Part II

- The autocorrelation function shows that a `\(MA(q)\)` process has a "memory" of only `\(q\)` periods. For example, if `\(q=1\)`, then the model has a memory of only one period. 
- The current value is not affected by values older than `\(q\)` periods.

## Implication for model identification 

- If the underlying process is **actually** a `\(MA(q)\)` process, then: 
- The `\(ACF\)` of a `\(MA(q)\)` model drops off completely after `\(q\)` periods. 
- This means that the `\(ACF\)` provides a considerable amount of information about the order of dependence if the process is a `\(MA\)` process. 

---
class: inverse, center, middle 
# Simulation of MA(q) Models

---
# MA(1) Models: Time Series Plots, Part 0

- `\(MA\)` models have a "limited" memory, which we can see in the formula for its autocorrelation. 
- For `\(MA(1)\)` models, this memory lasts only for one period. 
- As a result, **the difference in dynamics for `\(MA\)` models are not as notable as the dynamics in `\(AR(1)\)` processes when values are different.** 
  - Recall that `\(AR(1)\)` models with positive parameters tended to have "runs" 
  - Further, recall that `\(AR(1)\)` models with negative parameters tended to "oscillate" 
 
---
# AR(1) Example Plots 

![](unit_08_files/figure-html/unnamed-chunk-23-1.png)&lt;!-- --&gt;

---
# MA(1) Models: Time Series Plots

- The different dynamics of the `\(MA\)` models with positive and negative coefficients come from the positive and negative autocorrelation. 



![](unit_08_files/figure-html/unnamed-chunk-25-1.png)&lt;!-- --&gt;

---
# MA(1) Models: ACF Plots 

1. The `\(ACF\)` drops sharply after the **first lag** 
2. This pattern holds, no matter the value of the `\(MA\)` parameter, `\(\theta\)`. 
3. For `\(MA(1)\)` models, if `\(\theta &gt; 0\)`, then there is a positive correlation; and if `\(\theta &lt; 0\)` leads to a negative correlation.

![](unit_08_files/figure-html/unnamed-chunk-26-1.png)&lt;!-- --&gt;

---
# MA(1) Models: PACF Plots 

1. Unlike the `\(ACF\)`, the `\(PACF\)` of `\(MA\)` models decay gradually to zero. 
2. We can see the cause for this in the **infinite autoregressive representation** of the `\(MA(1)\)` process. 
3. For a `\(MA(1)\)` model, if `\(\theta &gt; 0\)` the delay oscillates to zero; if `\(\theta &lt; 0\)` the decay is "mostly" one-sided. 



![](unit_08_files/figure-html/unnamed-chunk-28-1.png)&lt;!-- --&gt;


---
# MA(2) Models: Time Series Plots, Part I

- `\(MA(2)\)` models can allow for richer dynamics than those of `\(MA(1)\)` models. These richer dynamics can be used to improve forecasts. 
- However, based solely on the time series plots, it can be difficult to distinguish between `\(MA(1)\)` and `\(MA(2)\)` models that have the same value on the first `\(MA\)` parameter, i.e. `\(\theta_{1}\)`. 





![](unit_08_files/figure-html/unnamed-chunk-31-1.png)&lt;!-- --&gt;


# MA(2) Models: Time Series Plots, Part II

- Negative `\(MA\)` parameters produce more volatile series 
- The first `\(MA\)` parameter has a larger impact on the volatility than the second paramater. 

![](unit_08_files/figure-html/unnamed-chunk-32-1.png)&lt;!-- --&gt;

---
# MA(2) Models: Time Series and ACF Plots, Part I

- Note the impact the second `\(MA\)` paramter has on autocorrelation. 



![](unit_08_files/figure-html/unnamed-chunk-34-1.png)&lt;!-- --&gt;

---
# MA(2) Models: Time Series and ACF Plots, Part II

- The first autocorreleation is negative, influenced by the `\(MA\)` parameters of the model. 

![](unit_08_files/figure-html/unnamed-chunk-35-1.png)&lt;!-- --&gt;

---
# MA(2) Models: Time Series and ACF Plots, Part III
## Comparing the ACF of four different MA(2) models

![](unit_08_files/figure-html/unnamed-chunk-36-1.png)&lt;!-- --&gt;

---
class: inverse, center, middle 
# Modeling Using Real-World Data: Data and Descriptive Statistics 

---
# Data for this Example 
- Data is the `\(US \leftrightarrow NZ\)`  exchange rate from the St. Louis Fed Website. 
- With the raw data, we can create transformations that we need. 
- Or, one could use the library `quantmod` which is designed for the streaming of finance and time series data. 
- Data is **not** seasonally adjusted. 

![exchange rate plot](./figures/fred_exchange_rate_us_nz.png)

---
# Basic Data Structure


```r
us_nz &lt;- read_csv('./data/EXUSNZ.csv', show_col_types = FALSE) %&gt;% 
  mutate(time_index = yearmonth(ymd(DATE))) %&gt;% 
  select(time_index, EXUSNZ) %&gt;% 
  as_tsibble(index=time_index)

glimpse(us_nz)
```

```
## Rows: 532
## Columns: 2
## $ time_index &lt;mth&gt; 1971 Jan, 1971 Feb, 1971 Mar, 1971 Apr, 1971 May, 1971 Jun,…
## $ EXUSNZ     &lt;dbl&gt; 1.119, 1.125, 1.125, 1.125, 1.125, 1.125, 1.125, 1.133, 1.1…
```

---
# Exploratory Data Analysis 
## Time series plot


```r
us_nz %&gt;% 
  autoplot(.vars=EXUSNZ) + 
  labs(
    title = TeX('US$ \\leftrightarrow $NZ Exchange Rate'), 
    x = 'Time', y  = 'Exchange Rate')
```

![](unit_08_files/figure-html/unnamed-chunk-38-1.png)&lt;!-- --&gt;

---
# Exploratory Data Analysis
## Histogram of Values 


```r
us_nz %&gt;% 
  ggplot(aes(x=EXUSNZ)) + 
  geom_histogram(bins = 10) + 
  labs(title = 'Histogram of Exchange Rate', x=NULL, y=NULL)
```

![](unit_08_files/figure-html/unnamed-chunk-39-1.png)&lt;!-- --&gt;

---
# Exploratory Data Analysis
## ACF plot 


```r
us_nz %&gt;% 
  ACF(y = EXUSNZ) %&gt;% 
  autoplot() + 
  labs(title = 'ACF Plot', x = 'Lags', y = 'ACF')
```

![](unit_08_files/figure-html/unnamed-chunk-40-1.png)&lt;!-- --&gt;

---
# Exploratory Data Analysis
## PACF plot 


```r
us_nz %&gt;% 
  PACF(y = EXUSNZ) %&gt;% 
  autoplot() + 
  labs(title = 'PACF Plot', x='Lags', y='PACF')
```

![](unit_08_files/figure-html/unnamed-chunk-41-1.png)&lt;!-- --&gt;

---
class: inverse, center, middle 
# Modeling Using Real World Data 

---
# Modeling Using Real World Data 
## Fit the model


```r
us_nz_model &lt;- us_nz %&gt;% 
  model(arima = ARIMA(EXUSNZ ~ 1 + pdq(0,0,4) + PDQ(0,0,0))) %&gt;% 
  report()
```

```
## Series: EXUSNZ 
## Model: ARIMA(0,0,4) w/ mean 
## 
## Coefficients:
##          ma1     ma2     ma3     ma4  constant
##       2.0790  2.3060  1.6613  0.6125    0.7502
## s.e.  0.0528  0.0825  0.0489  0.0321    0.0133
## 
## sigma^2 estimated as 0.001636:  log likelihood=951.5
## AIC=-1891   AICc=-1891   BIC=-1865
```

```r
us_nz_model_data &lt;- augment(us_nz_model)
```

---
# Modeling Using Real World Data 
## Examine Model Diagnostics: Residuals vs. Fitted 


```r
us_nz_model_data %&gt;% 
  ggplot() + 
  aes(x=.resid, y=.fitted) + # note the error in the async slides
                             # the labels are flipped. these labels are correct
  geom_point()
```

![](unit_08_files/figure-html/unnamed-chunk-43-1.png)&lt;!-- --&gt;

---
# Modeling Using Real World Data 
## Examine Model Diagnostics: Time vs. Residuals


```r
us_nz_model_data %&gt;% 
  ggplot(aes(x=time_index, y = .resid)) + 
  geom_line()
```

![](unit_08_files/figure-html/unnamed-chunk-44-1.png)&lt;!-- --&gt;

---
# Modeling Using Real World Data 
## Examine Model Diagnostics: ACF of Residuals 


```r
us_nz_model_data %&gt;% 
* ACF(y=.resid) %&gt;%
  autoplot()
```

![](unit_08_files/figure-html/unnamed-chunk-45-1.png)&lt;!-- --&gt;

---
# Modeling Using Real World Data 
## Examine Model Diagnostics: PACF of Residuals 


```r
us_nz_model_data %&gt;% 
* PACF(y=.resid) %&gt;%
  autoplot()
```

![](unit_08_files/figure-html/unnamed-chunk-46-1.png)&lt;!-- --&gt;

---
class: inverse, center, middle
# Simulated Data: A First Forecast 

---
# Estimation Results of a MA(2) Model 

- Create simulated data from a `\(MA(2)\)` process, with parameters `\(\theta_{1} = 0.5\)` and `\(\theta_{2} = -0.4\)`. 
- Estimate a model against this data and evalute whether the model recovers the correct parameter values for `\(\theta\)`. 


```r
ma_2_data &lt;- arima.sim(n=100, model=list(ma=c(0.5, -0.4))) %&gt;% 
  as_tsibble() 

ma_2_estimated_model &lt;- ma_2_data %&gt;% 
  model(ARIMA(formula = value ~ 1 + pdq(0,0,2) + PDQ(0,0,0))) %&gt;% 
  report() 
```

```
## Series: value 
## Model: ARIMA(0,0,2) w/ mean 
## 
## Coefficients:
##          ma1      ma2  constant
##       0.5258  -0.4742    0.1021
## s.e.  0.0846   0.0812    0.1178
## 
## sigma^2 estimated as 1.281:  log likelihood=-154.8
## AIC=317.6   AICc=318   BIC=328
```

- The estimated coefficients are not distinguishable from their true values

---
# Examine Results 


```r
ma_2_model_data &lt;- ma_2_estimated_model %&gt;% 
  augment()

ma_2_model_data %&gt;% 
  select(value, .fitted, .resid)
```

```
## # A tsibble: 100 x 4 [1]
##      value .fitted   .resid index
##      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;
##  1 -0.551  -0.0180 -0.533       1
##  2 -1.99   -0.353  -1.64        2
##  3  0.519  -0.109   0.628       3
##  4  1.21    0.990   0.215       4
##  5 -0.0846 -0.0807 -0.00382     5
##  6  0.673   0.0536  0.619       6
##  7  2.22    0.476   1.74        7
##  8  2.49    0.689   1.80        8
##  9  0.199   0.129   0.0700      9
## 10  1.85   -0.561   2.41       10
## # … with 90 more rows
```

---
# Examine Diagnostics 



![](unit_08_files/figure-html/unnamed-chunk-50-1.png)&lt;!-- --&gt;

---
# Original vs. Estiamted Series

![](unit_08_files/figure-html/unnamed-chunk-51-1.png)&lt;!-- --&gt;

- Notice that the predictions are almost never as extreme as the observed values.

---
# Forecasting 

- The following graph plots the original series and adds predictions for 20 periods into the future
  - In light and dark blue are the 80% and 95% confidence intervals. 


```r
ma_2_data %&gt;% 
  model(ARIMA(formula = value ~ 1 + pdq(0,0,2) + PDQ(0,0,0))) %&gt;% 
  forecast(h=20) %&gt;% 
  autoplot(ma_2_data)
```

![](unit_08_files/figure-html/unnamed-chunk-52-1.png)&lt;!-- --&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
