---
title: "w271: Categorical, Time Series, and Panel Regression"
subtitle: "Autoregressive and Moving Average Models"
author: "Jeffrey Yau"
institute: "UC Berkeley, School of Information"
date: "Updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts] 
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, echo=FALSE, message=FALSE, include=FALSE}
library(latex2exp)
library(tidyverse)
library(ggplot2)
library(patchwork)
library(data.table)

library(magrittr)
library(patchwork)

library(tsibble)
library(fpp3)
library(feasts)
library(fable)
library(astsa)

library(lubridate)
library(lmtest)

berkeley_blue   <- '#003262'
california_gold <- '#FDB515'

options(htmltools.dir.version = FALSE)
options(digits = 4)
knitr::opts_chunk$set(echo = FALSE, dpi = 300, fig.height = 3)


theme_set(theme_minimal())
```

class: inverse, center, middle 

# Mathematical Formulation and Properties of ARMA Models 

---

# Mathematical Formulation of ARMA(p,q) Models, Part 1

A time series $x_{t}: \dots , -2, -1, 0, 1,2, \dots$ is called a **mixed autoregressive moving average process** of order $(p,q)$, denoted $ARMA(p,q)$ if it is stationary and takes the following functional form: 

$$x_{t} = \phi_{1}x_{t-1} + \dots + \phi_{p}x_{t-p} + \omega_{t} - \theta_{1}\omega_{t-1} - \dots - \theta_{q}\omega_{t-q}$$

where $\phi_{p} \neq 0, \theta_{q} \neq 0$, and $\sigma_{\omega}^{2} > 0$. Also, we implicitly assume that the series $x_{t}$ is demeaned: $x_{t} - \mu$. To simplify notation, we do not use $\tilde{x}$ where $\tilde{x} = x_{t} - \mu$. 

The parameters $p$ and $q$ are respectively called the *autoregressive* and *moving average* orders. 

To incorporate a non-zero mean, $\mu$ into the model, we set $\alpha = \mu(1 - \phi_{1} - \dots - \phi_{p})$ and re-write the model as: 

$$x_{t} = \alpha + \phi_{1}x_{t-1} + \dots + \phi_{p}x_{t-p} + \omega_{t} + \theta_{1}\omega_{t-1} + \dots + \theta_{q}\omega_{t-q}$$

where $\omega_{t}$ is assumed to be a Gaussian white noise time series with mean zero and variance $\sigma_{\omega}^{2}$. 

---

# Mathematical Formulation of ARMA(p,q) Models, Part 2

Using $AR$ and $MA$ operators defined in the last lecture, the $ARMA(p,q)$ model can be expressed concisely using the Backshift operator: 

$$\phi(B)x_{t} = \theta(B)\omega_{t}$$

Because the process is stationary, its mean is constant over time: 

$$\mu = \alpha + \phi_{1}\mu + \dots + \phi_{p}\mu$$ 

which can be rewritten as: 

$$\mu = \frac{\alpha}{1 - \phi_{1} - \dots - \phi_{p}}$$

---

# Properties of the ARMA Model: ARMA(1,1)

To study the properties of the $ARMA(p,q)$ model, first consider the $ARMA(1,1)$ model: 

$$x_{t} = \phi x_{t-1} + \omega_{t} + \theta\omega_{t-1}$$

where $\{\omega_{t} \} \sim WN(0, \sigma^{2})$ and $\phi + \theta \neq 0$. 

Rewriting the model using the Backshift operator: 

$$\phi(B)x_{t} = \theta(B)\omega_{t}$$

where $\phi(B)$ and $\theta(B)$ are the linear filters. 

$$\phi(B) = 1 - \phi(B) \text{ and } \theta(B) = 1 + \theta(B) $$

where the bases of $\phi(z)$ lie outside the unit circle, so the model is stationary. 

## In other words: 

In the simple $ARMA(1,1)$ model, a stationary solution exists if and only if $\phi \neq \{1, -1\}$. 

---

# Mean, Variance, and Autocovariance of ARMA(1,1) Model 

Write the process using the infinate $MA$ representation: 

$$x_{t} = \sum_{j=0}^{\infty} \psi_{j}\omega_{t-j}$$

It follows immediately that $E[x_{t}] = 0$. 

The autocovariance function is: 

$$\gamma(h) = cov(x_{t+h}, x_{t}) = \sigma_{\omega}^{2} \sum_{j=0}^{\infty}\psi_{j}\psi_{j+h}, \quad h \geq 0$$

---

# Autocorrelation Function of the ARMA(1,1) Model, Part 1 

The autocovariance function satisfies the following statement: 

$$\gamma(h) - \phi\gamma(h-1) = 0, \quad h = 2, 3, \dots$$

And, with some algebra, we can write this in the following form: 

$$\gamma(h) = \frac{\gamma(1)}{\phi}\phi_{h} = \sigma_{\omega}^{2} \frac{(1 + \theta\phi)(\phi + \theta)}{(1 - \phi^{2})}\phi^{h-1}$$

The autocorrelation function at time lag $h$ is the ratio of autocovariance at that time $h$ divided by the variance: 

$$\rho(h) = \frac{\gamma(h)}{\gamma(0)}$$

$$\rho(h) = \frac{(1 + \theta\phi)(\phi + \theta)}{(1 - \phi^{2})}\phi^{h-1}, \quad h \geq 1$$

---

# Autocorrelation Function of the ARMA(1,1) Model, Part 2

The $ACF$ of an $AR(1)$ model takes the following form: 

$$\rho(h) = \frac{\gamma(h)}{\gamma(0)} = \phi^{h}, \quad h \geq 0$$

- Note that the general form of the autocorrelation functions between the two models are **not** that different from each other. 
- For this reason, it is very hard to distinguish an $ARMA(1,1)$ and an $AR(1)$ model from one another based only on the sample $ACF$. 

---

class: inverse, center, middle 

# Comparing ARMA Models and AR Models Using Simulated Series, Part 1

---

# ACF or ARMA and AR Models: Recap

- Recall that the $ACF$ or $ARMA$ models and $AR$ models are hard to distinguish. 
- Their functional forms are: 

## ACF of ARMA(1,1) Model 

$$ \rho(h) = \frac{(1 + \theta\phi)(\phi + \theta)}{(1 - \phi^{2})}\phi^{h-1}, \quad h \geq 1$$

## ACF of AR(1) Model 

$$\rho(h) = \frac{\gamma(h)}{\gamma(0)} = \phi^{h}, \quad h \geq 0$$

- These functional forms differ only by a constant multiple. 
- Therefore, using the ACF alone is insufficient for model "identification" 

---

# Time Series Plots of AR(1) and ARMA(1,1) Models, Part 1 

- All simulations in this module have 100 simulated points. 
- If the $AR$ parts of the $AR(1)$ and $ARMA(1,1)$ models are identical, it is **very difficult** to distinguish between the two based only on the time-plots. 

```{r ar vs arma}
## make ar data
ar_data_06 <- arima.sim(n=100, model=list(ar=0.6))  %>% as_tsibble()
ar_data_n6 <- arima.sim(n=100,model=list(ar=-0.6))  %>% as_tsibble()
ar_data_08 <- arima.sim(n=100, model=list(ar=0.8))  %>% as_tsibble() 
ar_data_n8 <- arima.sim(n=100, model=list(ar=-0.8)) %>% as_tsibble()
  
## make arma data
arma_data_0605 <- arima.sim(n=100, model=list(ar=0.6, ma=0.5))  %>% as_tsibble() 
arma_data_06n5 <- arima.sim(n=100, model=list(ar=0.6, ma=-0.5)) %>% as_tsibble() 
arma_data_0805 <- arima.sim(n=100, model=list(ar=0.8, ma=05))   %>% as_tsibble() 
arma_data_08n5 <- arima.sim(n=100, model=list(ar=0.8, ma=-0.5)) %>% as_tsibble()
```

```{r create ar and arma plots}
plot_ts_ar06 <- ar_data_06 %>% autoplot(.vars=value)
plot_ts_arn6 <- ar_data_n6 %>% autoplot(.vars=value)
plot_ts_ar08 <- ar_data_08 %>% autoplot(.vars=value)
plot_ts_arn8 <- ar_data_n8 %>% autoplot(.vars=value)

plot_ts_arma_0605 <- arma_data_0605 %>% autoplot(.vars=value)
plot_ts_arma_06n5 <- arma_data_06n5 %>% autoplot(.vars=value)
plot_ts_arma_0805 <- arma_data_0805 %>% autoplot(.vars=value)
plot_ts_arma_08n5 <- arma_data_08n5 %>% autoplot(.vars=value)
```

```{r layout plots}
(plot_ts_ar06 | plot_ts_arn6 | plot_ts_arma_0605 | plot_ts_arma_06n5) / 
  (plot_ts_ar08 | plot_ts_arn8 | plot_ts_arma_0805 | plot_ts_arma_08n5)
```

---

# Time Series Plots of AR(1) and ARMA(1,1) Models, Part 2

- Adding an additional $AR$ and/or $MA$ term does not help to distinguish the model. 
- And, adding an additional $AR$ term with a negative coefficient makes the distinction harder. 

```{r make more data}
arma_data_06n5_05   <- arima.sim(n=100, model=list(ar=c(0.6, -0.5),  ma=0.5))          %>% as_tsibble() 
arma_data_n6n5_05n2 <- arima.sim(n=100, model=list(ar=c(-0.5, -0.5), ma=c(0.5, -0.2))) %>% as_tsibble() 
arma_data_0603_05n2 <- arima.sim(n=100, model=list(ar=c(0.6, 0.3),   ma=c(0.5, -0.2))) %>% as_tsibble() 
arma_data_0801_05n2 <- arima.sim(n=100, model=list(ar=c(0.8, 0.1),   ma=c(0.5, -0.2))) %>% as_tsibble() 
```

```{r create plots of more data}
plot_ts_06n5_05   <- arma_data_06n5_05   %>% autoplot(.vars=value)
plot_ts_n6n5_05n2 <- arma_data_n6n5_05n2 %>% autoplot(.vars=value)
plot_ts_0603_05n2 <- arma_data_0603_05n2 %>% autoplot(.vars=value)
plot_ts_0801_05n2 <- arma_data_0801_05n2 %>% autoplot(.vars=value)
```

```{r layout more plots}
(plot_ts_ar06 | plot_ts_arn6 | plot_ts_06n5_05 | plot_ts_n6n5_05n2) / 
  (plot_ts_ar08 | plot_ts_arn8 | plot_ts_0603_05n2 | plot_ts_0801_05n2)
```

---

# ACF of AR(1) and ARMA(1,1) Models 

- As seen from the theoretical $ACF$, using a correlogram alone cannot distinguish $AR$ and $ARMA$ models. 
- We'll examine the empirical $ACF$ from the simulated models. 

```{r create ACF plots}
plot_acf_06 <- ar_data_06 %>% ACF(y=value) %>% autoplot() 
plot_acf_n6 <- ar_data_n6 %>% ACF(y=value) %>% autoplot() 
plot_acf_08 <- ar_data_08 %>% ACF(y=value) %>% autoplot() 
plot_acf_n8 <- ar_data_n8 %>% ACF(y=value) %>% autoplot()

plot_acf_0605 <- arma_data_0605 %>% ACF(y=value) %>% autoplot() 
plot_acf_06n5 <- arma_data_06n5 %>% ACF(y=value) %>% autoplot() 
plot_acf_0805 <- arma_data_0805 %>%ACF(y=value) %>% autoplot() 
plot_acf_08n5 <- arma_data_08n5 %>% ACF(y=value) %>% autoplot() 

plot_acf_06n5_05   <- arma_data_06n5_05   %>% ACF(y=value) %>% autoplot() 
plot_acf_n6n5_05n2 <- arma_data_n6n5_05n2 %>% ACF(y=value) %>% autoplot() 
plot_acf_0603_05n2 <- arma_data_0603_05n2 %>% ACF(y=value) %>% autoplot() 
plot_acf_0801_05n2 <- arma_data_0801_05n2 %>% ACF(y=value) %>% autoplot() 
```

```{r layout acf plots}
(plot_acf_06 | plot_acf_n6 | plot_acf_0605 | plot_acf_06n5) / 
  (plot_acf_08 | plot_acf_n8 | plot_acf_0805 | plot_acf_08n5) 
```

---

# PACF of AR(1) and ARMA(1,1) Models 

- The $PACF$ of $AR(p)$ models sharply drop off after $p$ lags while the $PACF$ of $ARMA(p,q)$ models generally gradually decline to zero. 
- But, this isn't a guarantee, and the slides in the async do not produce an observable difference. 

```{r create pacf plots}
plot_pacf_06 <- ar_data_06 %>% PACF(y=value) %>% autoplot() 
plot_pacf_n6 <- ar_data_n6 %>% PACF(y=value) %>% autoplot() 
plot_pacf_08 <- ar_data_08 %>% PACF(y=value) %>% autoplot() 
plot_pacf_n8 <- ar_data_n8 %>% PACF(y=value) %>% autoplot()

plot_pacf_0605 <- arma_data_0605 %>% PACF(y=value) %>% autoplot()
plot_pacf_06n5 <- arma_data_06n5 %>% PACF(y=value) %>% autoplot()
plot_pacf_0805 <- arma_data_0805 %>% PACF(y=value) %>% autoplot()
plot_pacf_08n5 <- arma_data_08n5 %>% PACF(y=value) %>% autoplot()
```

```{r layout of pacf plots}
(plot_pacf_06 | plot_pacf_n6 | plot_pacf_0605 | plot_pacf_06n5) / 
  (plot_pacf_08 | plot_pacf_n8 | plot_pacf_0805 | plot_pacf_08n5)
```

---

# PACF of AR(1) and ARMA(1,1) Models 

- With higher-order series, there is a similar difficulty in observing differences between series. 

```{r create higher-order pacf plots}
plot_pacf_06n5_05   <- arma_data_06n5_05   %>% PACF(y=value) %>% autoplot() 
plot_pacf_n6n5_05n2 <- arma_data_n6n5_05n2 %>% PACF(y=value) %>% autoplot() 
plot_pacf_0603_05n2 <- arma_data_0603_05n2 %>% PACF(y=value) %>% autoplot() 
plot_pacf_0801_05n2 <- arma_data_0801_05n2 %>% PACF(y=value) %>% autoplot() 
```

```{r layout higher-order pacf plots}
(plot_pacf_06n5_05 | plot_pacf_n6n5_05n2) / 
  (plot_pacf_0603_05n2 | plot_pacf_0801_05n2)
```

---

class: inverse, center, middle

# Comparing ARMA Models Using Simulated Series, Part 2: Model Identification

---

# AR vs. ARMA Models: Another Example 

- One more example with three $AR(1)$ models and one $ARMA(2,1)$ model

```{r make arma data for final set, echo = TRUE}
set.seed(898)

ar1_1 <- arima.sim(n=100, model=list(ar=0.4))                 %>% as_tsibble()
ar1_2 <- arima.sim(n=100, model=list(ar=0.8))                 %>% as_tsibble() 
ar1_3 <- arima.sim(n=100, model=list(ar=0.9))                 %>% as_tsibble() 
ar2_1 <- arima.sim(n=100, model=list(ar=c(0.6, 0.3), ma=0.5)) %>% as_tsibble()
```

- The closer the $AR$ parameter to $1$, the more persistent the series. 
- Both $AR(ar=0.9)$ and $ARMA(ar=0.6, ma=0.3)$ can produce very persistent series

---

# AR vs. ARMA Models: Another Example 

```{r make arma plots for final set}
plot_ar1_1 <- arima.sim(n=100, model=list(ar=0.4)) %>% as_tsibble() %>% autoplot(.vars=value)
plot_ar1_2 <- arima.sim(n=100, model=list(ar=0.8)) %>% as_tsibble() %>% autoplot(.vars=value)
plot_ar1_3 <- arima.sim(n=100, model=list(ar=0.9)) %>% as_tsibble() %>% autoplot(.vars=value)
plot_ar2_1 <- arima.sim(n=100, model=list(ar=c(0.6, 0.3), ma=0.5)) %>% 
  as_tsibble() %>% autoplot(.vars=value)
```

```{r layout arma plots for final set}
(plot_ar1_1 | plot_ar1_2) / 
  (plot_ar1_3 | plot_ar2_1)
```

---

# AR vs. ARMA Models, ACF

```{r}
plot_acf_ar1_1 <- arima.sim(n=100, model=list(ar=0.4)) %>% as_tsibble() %>% ACF(y=value) %>% autoplot()
plot_acf_ar1_2 <- arima.sim(n=100, model=list(ar=0.8)) %>% as_tsibble() %>% ACF(y=value) %>% autoplot()
plot_acf_ar1_3 <- arima.sim(n=100, model=list(ar=0.9)) %>% as_tsibble() %>% ACF(y=value) %>% autoplot()
plot_acf_ar2_1 <- arima.sim(n=100, model=list(ar=c(0.6, 0.3), ma=0.5)) %>% 
  as_tsibble() %>% ACF(y=value) %>% autoplot()
```

```{r}
(plot_acf_ar1_1 | plot_acf_ar1_2) / 
  (plot_acf_ar1_3 | plot_acf_ar2_1)
```


---

# AR vs. ARMA Models: PACF

```{r}
plot_pacf_ar1_1 <- arima.sim(n=100, model=list(ar=0.4)) %>% as_tsibble() %>% PACF(y=value) %>% autoplot()
plot_pacf_ar1_2 <- arima.sim(n=100, model=list(ar=0.8)) %>% as_tsibble() %>% PACF(y=value) %>% autoplot()
plot_pacf_ar1_3 <- arima.sim(n=100, model=list(ar=0.9)) %>% as_tsibble() %>% PACF(y=value) %>% autoplot()
plot_pacf_ar2_1 <- arima.sim(n=100, model=list(ar=c(0.6, 0.3), ma=0.5))  %>% 
  as_tsibble() %>% PACF(y=value) %>% autoplot()
```

```{r}
(plot_pacf_ar1_1 | plot_pacf_ar1_2) / 
  (plot_pacf_ar1_3 | plot_pacf_ar2_1)
```


---

# ARMA-Type Model Identification 

- In general, the $ACF$ and $PACF$ of $AR(p)$, $MA(q)$, and $ARMA(p,q)$ models have the following characteristics. 

|        | $AR(p)$                | $MA(q)$                | $ARMA(p,q)$ |
|--------|------------------------|------------------------|-------------|
| $ACF$  | Tails off              | Cuts off after lag $q$ | Tails Off   |
| $PACF$ | Cuts off after lag $p$ | Tails off              | Tails Off   |

- As we have seen in the simulated examples, these features can be used only to narrow down the possibilities of processes underlying a realization of the observed series
- In practice, we typically estimate a series of $ARMA$ models of different orders, and use $AIC$ and $BIC$ (and maybe even forecast performance) to choose a model. 

---

class: inverse, center, middle

# Modeling the BPD - NZD Exchange Rate, Part 1

---

# The Data: Basic Structure 

- This is the British pound $\leftrightarrow$ New Zealand dollar exchange rate series provided by *Introductory Time Series with R* (obtained from a GitHub port).
- It is an annual series with 39 observations and contains only the series itself (i.e. one variable) 

```{r}
data_gbp_nzd <- read.csv(file = './data/xrate.txt') %>% 
  mutate(index = 1:39) %>% 
  as_tsibble(index=index)
glimpse(data_gbp_nzd)
```

---

# Descriptive Statistics and Data Visualization 

- Similar to the USD $\leftrightarrow$ NZD exchange rate series, this series is very persistent; a feature that cannot be seen using only the histogram and descriptive statistics. 
- It does not appear to be captured well by a $MA$ model, but perhaps an $ARMA$ model will do better? 

```{r nzd plots}
plot_ts   <- data_gbp_nzd %>% autoplot(xrate)  
plot_hist <- data_gbp_nzd %>% ggplot(aes(x=xrate)) + geom_histogram(bins=10)
plot_acf  <- data_gbp_nzd %>% ACF(y=xrate) %>% autoplot()
plot_pacf <- data_gbp_nzd %>% PACF(y=xrate) %>% autoplot() 
```

```{r layout nzd plots}
(plot_ts | plot_hist) /
  (plot_acf | plot_pacf)
```

---

# Estimation: MA(5) Model 

- We will estimate a $MA$ model for comparison 
- The first four $MA$ parameters are all significant 
- Note that the last $MA$ parameter is not significant
- At this point, it is hard to judge how good the estimation is. To do so, we have to examine the residuals, visualize the in-sample fit, and the out-of-sample forecast. 

```{r}
model_ma_5 <- data_gbp_nzd %>% 
  model(ARIMA(formula = xrate ~ 1 + pdq(0, 0, 5) + PDQ(0, 0, 0)))

model_ma_5 %>% report()  
```

--- 

# Model Diagnostics 

- The time series plot of the residuals does not suggest a white noise sample path.
- However, both the ACF and PACF do not show any statistical significant autocorrelations.
- The null hypothesis that the series is not correlated cannot be rejected based on the Ljung-Box test. 

```{r}
data_gbp_nzd_augmented <- model_ma_5 %>% augment()

plot_residuals_ts  <- data_gbp_nzd_augmented %>% 
  autoplot(.vars=.resid)
plot_residuals_hist <- data_gbp_nzd_augmented %>% 
  ggplot(aes(x=.resid)) + 
  geom_histogram(bins=10)
plot_residuals_acf <- data_gbp_nzd_augmented %>% 
  ACF(y=.resid) %>% 
  autoplot()
plot_residuals_pacf <- data_gbp_nzd_augmented %>% 
  PACF(y=.resid) %>% 
  autoplot()
```

```{r}
(plot_residuals_ts | plot_residuals_hist) / 
  (plot_residuals_acf | plot_residuals_pacf)
```

```{r}
data_gbp_nzd_augmented %$% 
  ljung_box(.resid)
```

---

# Model Performance Evaluation: In-Sample Fit

- The following time series plot displays the original series, which has 39 observations, the estimated values (in blue), overlaid with the residual series (in green and right y-axis).
- In-sample fit “looks reasonable.”

```{r}
plot_actual_fitted_residuals <- data_gbp_nzd_augmented %>% 
  ggplot() + 
    geom_line(aes(x=index, y=xrate,      color = 'values')) + 
    geom_line(aes(x=index, y=.fitted,    color = 'fitted')) + 
    geom_line(aes(x=index, y=.resid+3,   color = 'residuals'), linetype = 'dashed') + 
    scale_y_continuous(sec.axis = sec_axis(~.-3, name = 'Residuals')) + 
    scale_color_manual(
      name = 'Series', 
      values = c(
        'values' = berkeley_blue, 
        'fitted' = california_gold, 
        'residuals' = 'darkred')) + 
  labs(title = 'Actual, Fitted, and Residuals Plot')

plot_actual_fitted_residuals
```

---

# Forecasting 

- This plot shows the original series, the estimated values, and a six-step ahead forecast and the forecasting intervals. 

```{r}
forecast(model_ma_5, h=6) %>% 
  autoplot(data_gbp_nzd)
```

---

# Back Testing and Out of Sample Testing 

- An alternative way to evaluate the model (or the class of model) under consideration is to use out-of-sample forecasting (or back-testing) by leaving out a subsample in the estimation. 
- Leaving out a subsample from a time series has to be done **carefully** because it is not sensible to just "randomly" drop observations from the series. **Doing so would break the dependency structure in the series.** 
- In the next backtesting example, we exclude the last-six observations (retaining the first 33 observations) from the sample and re-estimating the $MA(5)$ model. 
- Then, produce a 12-step ahead forecast. 
  - Because the last six observations are actually observed, we can compare the forecasts with actual values. 
  - This process is called *backtesting*. 
  
---

# Back-Testing and Out-of-Sample Forecasting 

- This shows a dramatic difference from the forecast using the entire series. 
- The forecast continues the downward trend in the observed series
- The forecast after the first five series stays flat (do you remember why?)

```{r, warning = FALSE}
model_backtesting <- data_gbp_nzd %>% 
  filter(index <= 33) %>% 
  model(ARIMA(formula = xrate ~ 1 + pdq(0,0,5) + PDQ(0,0,0)))

backtesting_plot_data <- rbind(
  ## backtesting data set 
  augment(model_backtesting) %>% 
    rename(.mean = .fitted) %>% 
    mutate(source = 'backtesting model') %>%
    select(index, xrate, .mean, source) %>% 
    as_tibble(), 
  ## original data set 
  data_gbp_nzd %>% 
    mutate(
      source = 'original data',
      .mean = NA) %>% 
    select(index, xrate, .mean, source), 
  ## forecasting dataset 
  model_backtesting %>% 
    forecast(h=12) %>% 
    as_tsibble() %>% 
    mutate(
      source = 'forecast', 
      xrate = NA) %>% 
    select(index, xrate, .mean, source) %>% 
    as_tibble()
  )

backtesting_plot_data %>% 
  ggplot() + 
  aes(x=index) + 
  geom_line(aes(y=xrate, color = source)) + 
  geom_line(aes(y=.mean, color = source))
```

---

class: inverse, center, middle 

# Modeling the BPD - NZD Exchange Rate, Part 2

---

# Estimation: ARMA(1,1) Model 

- Based on the time seiries, $ACF$, and $PACF$ plots, a low-order $ARMA$ model may do a better job than a higher-order "pure" $MA$ model. 
- Let's estimate an $ARMA(1,1)$ model.
- The $AR(p)$ and $MA(q)$ parameters are statistically significant 
- Note that the $AR(q)$ parameter is close to $1$. 

```{r}
model_arma_11 <- data_gbp_nzd %>% 
  model(ARIMA(xrate ~ 1 + pdq(1, 0, 1) + PDQ(0, 0, 0)))

model_arma_11 %>% report()
```

---

# Model Diagnostics 

- The $ACF$, $PACF$ and Lyung-Box test cannot reject the null-hypothesis that the residual series come sfrom a white noise process. 
- Note that these are underpowered tests -- there are only $39$ observations in total. 

```{r create arma(1-0-1) plots} 
plot_ts_resid <- model_arma_11 %>% 
  augment() %>% 
  autoplot(.vars=.resid)
plot_hist_resid <- model_arma_11 %>% 
  augment() %>%
  ggplot() + 
  aes(x=.resid) + 
  geom_histogram(bins=10)
plot_acf_resid <- model_arma_11 %>% 
  augment() %>% 
  ACF(y=.resid) %>% 
  autoplot() 
plot_pacf_resid <- model_arma_11 %>% 
  augment() %>% 
  PACF(y=.resid) %>% 
  autoplot() 
```

```{r layout arma(1-0-1) plots}
(plot_ts_resid | plot_hist_resid) / 
  (plot_acf_resid | plot_pacf_resid) 
```

--- 

# Model Performance Evaluation: In-Sample Fit

- Like the $MA(5)$ model, the ini-sample fit from the $ARMA(1,1)$ model looks "reasonable".

```{r}
model_arma_11 %>% 
  augment() %>%
  ggplot() + 
    geom_line(aes(x=index, y=xrate,      color = 'values')) + 
    geom_line(aes(x=index, y=.fitted,    color = 'fitted')) + 
    geom_line(aes(x=index, y=.resid+3,   color = 'residuals'), linetype = 'dashed') + 
    scale_y_continuous(sec.axis = sec_axis(~.-3, name = 'Residuals')) + 
    scale_color_manual(
      name = 'Series', 
      values = c(
        'values' = berkeley_blue, 
        'fitted' = california_gold, 
        'residuals' = 'darkred')) + 
  labs(title = 'Actual, Fitted, and Residuals Plot')
```

---

# Forecasting 

- Notice that the forecast still trends downward, although it does not decline as rapidly as that of the $MA(5)$ model. 

```{r}
forecast(model_arma_11, h=6) %>% 
  autoplot(data_gbp_nzd)
```

---

# Direct Comparison Between MA(5) and ARMA(1,1)

- Using the tools from `fpp3` we can fit several models, and compare their forecasting performance. 

```{r}
data_gbp_nzd %>% 
  model(
    ma_5    = ARIMA(xrate ~ 1 + pdq(0,0,5)   + PDQ(0,0,0)), 
    arma_11 = ARIMA(xrate ~ 1 + pdq(1,0,1) + PDQ(0,0,0))
  ) %>% 
  forecast(h=6) %>% 
  autoplot(data_gbp_nzd)
```

---

# Back-Testing and Out-of-Sample Forecasting 

- Re-estimate the $ARMA(1,1)$ model using on the first $33$ (of $39$) observations in the original series. 
- All of the parameters continue to be significant. 

```{r}
model_backtesting_arma <- data_gbp_nzd %>% 
  filter(index <= 33) %>% 
  model(ARIMA(formula = xrate ~ 1 + pdq(1,0,1) + PDQ(0,0,0)))

backtesting_plot_data_arma <- rbind(
  ## backtesting data set 
  augment(model_backtesting_arma) %>% 
    rename(.mean = .fitted) %>% 
    mutate(source = 'backtesting model') %>%
    select(index, xrate, .mean, source) %>% 
    as_tibble(), 
  ## original data set 
  data_gbp_nzd %>% 
    mutate(
      source = 'original data',
      .mean = NA) %>% 
    select(index, xrate, .mean, source), 
  ## forecasting dataset 
  model_backtesting_arma %>% 
    forecast(h=12) %>% 
    as_tsibble() %>% 
    mutate(
      source = 'forecast', 
      xrate = NA) %>% 
    select(index, xrate, .mean, source) %>% 
    as_tibble()
  )

backtesting_plot_data_arma %>% 
  ggplot() + 
  aes(x=index) + 
  geom_line(aes(y=xrate, color = source)) + 
  geom_line(aes(y=.mean, color = source))
```

---

# Not in Lecture

- There is a very sharp decline in the exchange rate between the 30th and 33 time periods 
- What would happen if our back testing had instead backtested against 10 periods? 

```{r}
backtesting_trials <- function(time_to_stop = 25) { 
  
  model_backtesting_arma <- data_gbp_nzd %>% 
    filter(index <= time_to_stop) %>% 
    model(ARIMA(formula = xrate ~ 1 + pdq(1,0,1) + PDQ(0,0,0)))
  
  backtesting_plot_data_arma <- rbind(
    ## backtesting data set 
    augment(model_backtesting_arma) %>% 
      rename(.mean = .fitted) %>% 
      mutate(source = 'backtesting model') %>%
      select(index, xrate, .mean, source) %>% 
      as_tibble(), 
    ## original data set 
    data_gbp_nzd %>% 
      mutate(
        source = 'original data',
        .mean = NA) %>% 
      select(index, xrate, .mean, source), 
    ## forecasting dataset 
    model_backtesting_arma %>% 
      forecast(h=39-time_to_stop) %>% 
      as_tsibble() %>% 
      mutate(
        source = 'forecast', 
        xrate = NA) %>% 
      select(index, xrate, .mean, source) %>% 
      as_tibble()
    )

  backtesting_plot_data_arma %>% 
    ggplot() + 
    aes(x=index) + 
    geom_line(aes(y=xrate, color = source)) + 
    geom_line(aes(y=.mean, color = source))
} 
```

```{r, warning=FALSE}
backtesting_trials(time_to_stop = 39-10)
```

---

class: inverse, center, middle 

# Review of Steps to Build ARMA Time Series Model 

---

# General Steps to Analyze a Time Series Model, Part 1  

1. Based on interaction of *theory*, *subject knowledge*, and *practice* consider a useful class of models. 
2. Collect and clean and structure the data. 
3. Conduct **exploratory time series data analysis** by plotting the series, examining the main patterns and atypical observations of the plots. 
  - What are the *trends* in the data? 
  - What are the *fluctuations around the trend* in the data? 
    - Is there seasonal variation? 
    - Is there other cyclical variation? 
  - Are there sharp changes in behavior (e.g. structural changes, or jumps)? 
  - Are there outliers in the data that deserve attention? 
4. Examine and statistically test whether the series is stationary (if applying a model that relies upon stationarity)

---

# # General Steps to Analyze a Time Series Model, Part 2

5. If the series is not stationary, transform it to a stationary series. 
  - Detrend, remove seasons, or apply a logarithmic or difference transformation
6. Model the stationary series with a stationary or integrated time series model. 
7. Examine the validity of the model's assumptions
  - *This is important!* 
  - If the model's assumptions are not satisfied, inference and forecasting hold no statistical guarantees, and **one should not proceed with statisical forecasting**. 
8. Among the valid models, choose the one that best satisfies the metric that you have pre-specified as your model selection metric
9. One you have chosen a (statistically) valid model, conduct statistical inference and/or forecasting. **Do so only if the underlying statistical assumptions are satisfied.**

---

# Modeling Procedures for ARMA Models 

When estimating $ARMA$ models using a time series, we use the following steps: 

1. Plot the data, identify the key dynamics and unusual observations. 
2. If necessary, transform the data (using a Box-Cox transformation) to stabilize the variance
3. If the data are not stationary: Take first differences until the data **is** stationary. 
4. Examine the $ACF$ and $PACF$: Is an $AR(p)$ or an $MA(q)$ model obviously appropriate? 
5. Try the chosen model, and use $AICc$ to search for models that might improve on your baseline model. 
6. Conduct diagnostic and formal model assumption testing: 
  - Check the residuals
  - Plot the the $ACF$ of the residuals 
  - Consider a portmanteau test of residuals. 
  - If they do **not** look like white noise, the model needs to be respecified, reestimated, and retested. 
7. Once the hypothesis that the residuals follow a white-noise process cannot be rejected, the model can be used for forecasting. 

---

class: inverse, center, middle 

# Nonstationary Models: An Introduction 

---

# Nonstationary Models, Part 1

- To this point, we have focused on stationary time series models and studied series that are **covariance stationary**. 
- In this lecture, we shift the focus to **nonstationary time series models**. 
- Many time series encountered in practice are nonstationary due to trends or seasonal effects 
## Good News! 

- Fortunately, many of the nonstationary series that we encounter can be convered to a covariance stationary series using simple *differencing*, especially using first-order differenci ng. 
- This modeling strategy leads to the *famous* **Box-Jenkins Approach** to drive the *Auto Regressive Integrated Moving Average*, or **ARIMA** models. 

---

# Nonstationary Models, Part 2

- The term **integrated** comes from the fact that the differenced series need to be aggregated to recover the original series. If this is the case, then the underlying process is called (i.e. is defined to be) an *integrated* series. 
- The *ARIMA* process can accomodate seasonal terms, which gives rise to the seasonal ARIMA (**SARIMA**) model. 
- Not all nonstationarity can be overcome with differencing. 
  - Volatility clustering, which occurs in many financial and macroeconomic time series generates *conditional heteroskedasticty*. 
  - Data of this form is better modeled using an *Autoregression Conditional Heteroskedastic* (ARCH) or *Generalized ARCH* (GARCH) model.
  
--- 

# Nonstationary Models, Part 3

- Without seasonal effects, first differencing can remove both *stochastic* and *deterministic* trends. 
  - Stochastic trends are processes like random walks
  - Deterministic trends are process like a linear trend. 
  
- Recall that a random walk taks the following form: 

$$y_{t} = y_{t-1} + \omega_{t}$$ 

- If data follows a random walk process (with the form written above), then taking a first difference transforms the model to the following form: 

$$\begin{aligned} 
\bigtriangledown y_{t} &= y_{t} - y_{t-1} \\ 
  &= \left(y_{t-1} + \omega_{t}\right) - \left(y_{t-1} - \omega_{t-1}\right) \\ 
  &= \omega_{t} - \omega_{t-1}
\end{aligned}$$

- Because there is no covariance between $\omega_{t}$ and $\omega_{t-1}$ this difference is itself a white noise process, with mean zero. And, so the first differencing has produced a mean-zero, stationary white-noise series. 

---

# Nonstationary Model, Part 4 

What happens if one applies a first-differencing approach to a linear trend with white-noise errors? *They get a $MA(1)$ process! 

$$y_{t} = a + bt + \omega_{t}$$ 

Can be transformed into the following series using first differencing: 

$$\begin{aligned} 
\bigdowntriangle y_{t} &= y_{t} - y_{t-1} \\ 
  &= \left(a + bt + \omega_{t}\right) - \left(a + bt + \omega_{t-1} \right) \\ 
  &= b + \left(\omega_{t} - \omega_{t-1}\right)
\end{aligned}$$

- Another transformation that is possible is to directly subtract the trend (if known) from the series. This will also produce a white noise process, and might be more sensible. 

$$\begin{aligned} 
y_{t} &= a + bt + \omega_{t} \\ 
y_{t} - \text{ known trend} &= y_{t} - (a + bt) \\ 
  &= a + bt + \omega_{t} - (a + bt) \\ 
  &= \omega_{t}
\end{aligned}$$

## In Practice 

- Do not blindly apply first differencing methods, or higher-order differencing methods. 
- Investigate and understand the series so that you can ask **and answer** the question, "What is the meaning of this series, and what is the meaning of this transformed series?" 

---

class: inverse, center, middle 

# Random Walk Processes 

---

# Random Walk Processess: An Introduction 

- Random walks are nothing more than $AR(1)$ processes where the $AR$ parameter is exactly $1$. 

$$\begin{aligned} 
y_{t} &= \phi_{1}y_{t-1} + \epsilon_{t} \\ 
\epsilon_{t} & \sim WN(0, \sigma^{2})
\end{aligned}$$

## Random walks are important processes 
- They are the basis of many series in continuous-time finance
- They are the basis of many series is atmospheric data 

## Random walks are just that...random! 

- Note that random walks do not revert back to any constant level. 
- These are not *mean-reversion* processes. 
- Instead, they wander up and down, without a tendency to settle at any particular level. 
- Although they are ill-behaved (i.e. not covariance stationary), the first difference of a random walk **is** stationary white noise.

---

# A simulated white noise series 

$$\begin{aligned} 
y_{t} &= \phi_{1}y_{t-1} + \epsilon_{t} \\ 
\epsilon_{t} & \sim WN(0, \sigma^{2})
\end{aligned}$$

```{r echo=TRUE}
random_walk <- function(time_periods=100, variance=1){
  rw <- rep(NA, time_periods)
  rw[1] <- 0
  for(i in 2:time_periods) {rw[i] <- rw[i-1] + rnorm(1, 0, variance)}
  return(rw)
}

ggplot() + geom_line(aes(x=1:100, y=random_walk(100)))
```

---

# Random Walk with Drive Processes 

- Random walks with drift are essentially models of trend
- On average, the process grows by the drift in each period. 

$$\begin{aligned} 
y_{t} &= \delta + \phi_{1}y_{t-1} + \epsilon_{t} \\ 
\epsilon_{t} & \sim WN(0, \sigma^{2})
\end{aligned}$$

```{r, echo=TRUE}
random_walk_with_drift <- function(time_periods=100, drift=0.5, variance=1){
  rwd <- rep(NA, time_periods)
  rwd[1] <- 0
  for(i in 2:time_periods) {rwd[i] <- drift + rwd[i-1] + rnorm(1, 0, variance)}
  return(rwd)
}

ggplot() + geom_line(aes(x=1:100, y=random_walk_with_drift(100)))
```

---

# Random Walk With and Without Drift 

```{r}
ggplot() + 
  geom_line(aes(x=1:100, y=20+random_walk(100), color='Without Drift')) + 
  geom_line(aes(x=1:100, y=random_walk_with_drift(100), color='With Drift')) + 
  scale_color_manual(
    name = "Drifty?", 
    values=c('Without Drift' = berkeley_blue, 'With Drift' = california_gold)) + 
  labs(
    title = 'Random Walk Process with and without Drift',
    x = 'Time', y = 'Value'
  )
```

---

# Random Walk Processess 

- The drift parameter plays the same role as the slope parameter in deterministic linear trend models 
- On average, processes "grow" by the drift in each period 
- The random walk with drift is also called model of **stochastic trend** because its trend is driven by stochastic shocks
- The most distinctive feature of random walk processes is that shocks affect the series **permanently**. If a shock lowers the value of the series, the random walk has no tendency to rise again. It stays lower permanently, until a new shock comes. 
- (What does this mean for a stock portfolio?) 
- A $k-$unit shock moves the expected future path of the series by one-unit. 

---

class: inverse, center, middle

# Introduction to ARIMA Processess 

---

# The Expectation of Random Walk Processes 

- Suppose that a process starts at some time $0$ with some value $y_{0}$. Then, we can derive properties of the series

$$y_{t} = y_{0} + \sum_{i=1}^{t} \epsilon_{i}$$

$$\begin{aligned} 
E[y_{t}] &= E\left[y_{0} + \sum_{i=1}^{t} \epsilon_{i}\right] \\ 
  &= E[y_{0}] + E\left[\sum_{i=1}^{t}\epsilon_{i}\right] \\ 
  &= E[y_{0}] + \sum_{i=1}^{t}E[\epsilon_{i}] \\ 
  &= E[y_{0}] + \sum_{i=1}^{t}0 = \mathbf{E[y_{0}]}
\end{aligned}$$

---

# The Variance of Random Walk Process 

With the same series, we can drive the variance as the following 

$$\begin{aligned} 
V[y_{t}] &= V\left[y_{0} + \sum_{i=1}^{t} \epsilon_{i}\right] \\ 
  &= V\left[\sum_{i=1}^{t}\epsilon_{t} \right] \\ 
  &= \sum_{i=1}^{t} V[\epsilon_{i}] \quad \text{Because 0 covariance} \\
  &= \sum_{i=1}^{t} \sigma^{2} \\
  & = \mathbf{t\sigma^{2}}
\end{aligned}$$

## The variance of a random walk series grows without bound through time! 
---

# Random Walk with Drift

- Similar derivations to those we have just shown (do them on your own!) are available for processes with drift: 

$$\begin{aligned} 
y_{t}    &= t\delta + y_{0} + \sum_{i=1}^{t}\epsilon_{i} \\ 
E[y_{t}] &= y_{0} + t\delta \\ 
V[y_{t}] &= t\sigma^{2}
\end{aligned}$$ 

## The mean grows by the speed of the drift, and variance grows without bound.

---

# Integrated Processes: An Introduction 

- First differencing can transform non-stationary random walk processes to stationary white noise processes. 

## Simple Transformations 

- Define $I(d)$ to be an **Integrated Process** with order of $d$. Then, 
  - White noise is the most simple $I(0)$ process. 
  - The random walk with the most simple $I(1)$ process. 
- In practice, $I(0)$ and $I(1)$ cases have *by far* the most applications. 
  - One reason is that results are hard to understand, and harder to explain, once as series is differenced too many times. 
  
---

# Random Walk Process 

A time series, $y_{t}$ follows an $ARIMA(p,d,q)$ process if the $d^{th}$ differences of the $y_{t}$ series is an $ARMA(p,q)$ process. Using the *backshift* operator, these series can be expressed as

$$\phi_{p}(B)(1-B)^{d}y_{t} = \theta_{q}(B)\omega_{t}$$

where $\phi_{p}$ and $\theta_{q}$ are polynomials of orders $p$ and $q$ discussed in the previous lecturers. 

---

class: inverse, center, middle 

# ARIMA Model: Simulation 

---

# ARIMA: Algebra Before Simulation, Part 1 

- Following the approach of the last few lectures, in this lecture we will simulate an *ARIMA* model and examine the patterns it exhibits is a time series plot, an $ACF$ plot, and a $PACF$ plot. 
- When we specify the model, we are specifying the "true" (i.e. population) model. So, we can evaluate, given a particular sample, how close the estimates are to the "true" value. 
- Consider the following model: 

$$\begin{aligned} 
y_{t} &= 0.5y_{t-1} + y_{t-1} + 0.5 y_{t-2} + \omega_{t} + 0.3 \omega_{t-1} \\ 
\bigtriangledown y_{t} = (y_{t} - y_{t-1}) &= 0.5 (y_{t-1} - y_{t-2}) + \omega_{t} + 0.3 \omega_{t-1} \\ 
\bigtriangledown y_{t} - 0.5(y_{t-1} - y_{t-2}) &= \omega_{t} + 0.3 \omega_{t-1} \\ 
\bigtriangledown y_{t} - 0.5 \bigtriangledown y_{t-1} &= \omega_{t} + 0.3 \omega_{t-1}
\end{aligned}$$

---

# ARIMA: Algebra Before Simulation, Part 2

Using the backshift operator, this equation can be rearranged and expressed as an $ARIMA(1,1,1)$ model. 

$$\begin{aligned}
\bigtriangledown y_{t} - 0.5 \bigtriangledown y_{t-1} &= \omega_{t} + 0.3 \omega_{t-1} \\ 
(1 - 0.5 B) \bigtriangledown y_{t} &= (1 + 0.3B) \omega_{t} \\ 
\bigtriangledown y_{t} &= 0.5 \bigtriangledown y_{t-1} + \omega_{t} + 0.3 \omega_{t-1}
\end{aligned}$$

- After the first difference, the model becomes a stationary, $ARMA(1,1)$ model.

---

# Simulate the Data! 

```{r, echo=TRUE}
set.seed(898)
y     <- c(rnorm(2), rep(NA, 98)) # start with y1 and y2 being white noise 
omega <- c(rnorm(3), rep(NA, 97)) # start with three shocks 

for(t in 3:100) { 
  omega[t+1] <- rnorm(1) # make a shock for the next time period 
  y[t] <- 0.5*y[t-1] + y[t-1] - 0.5*y[t-2] + omega[t] + 0.3 * omega[t-1]
}
```

```{r}
series <- tsibble(
  time  = 1:100, 
  value = y, 
  index = time
)
glimpse(series)
```

---

# Data Visualization 

```{r create arima plots}
plot_ts <- series %>% 
  autoplot(.vars=value) + 
  labs(title = 'Simulated Series')
plot_first_diff_ts <- series %>% 
  mutate(diff_value = c(NA, diff(value))) %>% 
  autoplot(.vars=value) + 
  labs(title = 'First Difference of Series')
plot_acf <- series %>% 
  ACF(y=value) %>% 
  autoplot() + 
  labs(title = 'ACF of Series')
plot_first_diff_acf <- series %>% 
  mutate(diff_value = c(NA, diff(value))) %>% 
  ACF(y=diff_value) %>%
  autoplot() + 
  labs(title = 'ACF of First Difference Series')
plot_pacf <- series %>% 
  PACF(y=value) %>% 
  autoplot() + 
  labs(title = 'PACF of Series')
plot_first_diff_pacf <- series %>% 
  mutate(diff_value = c(NA, diff(value))) %>% 
  PACF(y=diff_value) %>% 
  autoplot() + 
  labs(title = 'PACF of First Difference Series')
```

```{r layout arima plots}
(plot_ts | plot_first_diff_ts) / 
  (plot_acf | plot_first_diff_acf) / 
  (plot_pacf | plot_first_diff_pacf)
```

---

class: inverse, center, middle 

# ARIMA Models: Modeling with Simulated Data 

```{r, echo = TRUE} 
fit_model <- series %>% 
  model(
    arima_model = ARIMA(value ~ 1 + pdq(1,1,1) + PDQ(0,0,0))
  )

fit_model %>% report()
```

---

# Model Diagnostics Using Residuals 

Both the graphical and Ljung-Box tests do not reject the residual series as a realization of a white noise process. 

```{r}
p1 <- fit_model %>% 
  augment() %>% 
  autoplot(.vars=.resid) + 
  labs(title = 'Residual Series')
p2 <- fit_model %>% 
  augment() %>% 
  ggplot() + 
  aes(x=.resid) + 
  geom_histogram(bins=10) + 
  labs(title = 'Histogram of Residudals')
p3 <- fit_model %>% 
  augment() %>% 
  ACF(y=.resid) %>% 
  autoplot() + 
  labs(title = 'ACF of Residuals')
p4 <- fit_model %>% 
  augment() %>% 
  PACF(y=.resid) %>% 
  autoplot() + 
  labs(title = 'PACF of Residuals')

(p1 | p2) / 
  (p3 | p4)
```

---

# Model Performance Evaluation 

```{r}
fit_model %>% 
  augment() %>% 
  ggplot() + 
  geom_line(aes(x=time, y=value, color = 'Original Series')) + 
  geom_line(aes(x=time, y=.fitted, color = 'Predicted Series')) + 
  geom_line(aes(x=time, y=.resid, color = 'Residuals'), linetype = 'dashed') + 
  scale_color_manual(
    name = 'Series', 
    values = c('Original Series' = berkeley_blue, 'Predicted Series' = california_gold, 'Residuals' = 'darkred'))
  
```

---

# Forecasting 

Although the model fit it good, the model produces forecasts that are a "flat line" 

```{r}
forecast(fit_model, h=12) %>% 
  autoplot(series)
```

---

# Back-Testing / Out-of-sample Forecasting 

- Here we are doing two modeling steps at once: 
  1. We are choosing to back-test; and, 
  2. We are changing the model. 
- In general, your process should move only one modeling choice at a time, so that you can observe the effects of each modeling choice, *in isolation*. 

- Estimate an $ARIMA(1,1,0)$ model. 
- Back-test against the last $10$ datapoints. 

```{r}
filtered_series <- series %>% 
  filter(time < 90)

backtest_model_fit <- filtered_series %>% 
  model(
    ARIMA(value ~ 1 + pdq(1,1,0) + PDQ(0,0,0))
  )

backtest_model_fit %>% 
  augment() %>% 
  ggplot() + 
  geom_line(aes(x=time, y=value, color = 'Original Series')) + 
  geom_line(aes(x=time, y=.fitted, color = 'Estimated Series')) + 
  geom_line(aes(x=time, y=.resid, color = 'Residuals'), linetype = 'dashed') + 
  scale_color_manual(
    name = 'Series', 
    values = c('Original Series' = berkeley_blue, 'Estimated Series' = california_gold, 'Residuals' = 'darkred'))
```

---

# Back-Testing / Out-of-Sample Forecasting 

- Although the fit is good, only the 1-step ahead forecast is close

```{r}
forecast(backtest_model_fit, h=10) %>% 
  autoplot(series)
```

